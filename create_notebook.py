# create_notebook.py
import json

def create_preprocessing_notebook():
    """Cria o notebook Jupyter completo do zero"""
    
    notebook_content = {
        "cells": [
            {
                "cell_type": "markdown",
                "id": "introducao",
                "metadata": {},
                "source": [
                    "# üéØ PR√â-PROCESSAMENTO DE DADOS\\n\\n",
                    "## OBJETIVOS\\n",
                    "- Tratamento de valores faltantes\\n",
                    "- Encoding de vari√°veis categ√≥ricas\\n", 
                    "- Tratamento de outliers\\n",
                    "- Normaliza√ß√£o dos dados\\n",
                    "- Feature engineering\\n",
                    "- Salvamento do dataset processado\\n\\n",
                    "## üìÅ ARQUIVOS GERADOS\\n",
                    "- `dataset_preprocessado.csv` - Dataset pr√©-processado\\n",
                    "- `models/scaler.pkl` - Scaler para novas predi√ß√µes"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "importacoes",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- IMPORTA√á√ïES NECESS√ÅRIAS ---\\n",
                    "import pandas as pd\\n",
                    "import numpy as np\\n",
                    "import matplotlib.pyplot as plt\\n",
                    "import seaborn as sns\\n",
                    "from sklearn.impute import SimpleImputer\\n",
                    "from sklearn.preprocessing import StandardScaler\\n",
                    "import joblib\\n",
                    "import os\\n\\n",
                    "# Configura√ß√µes de visualiza√ß√£o\\n",
                    "sns.set(style=\\\"whitegrid\\\", palette=\\\"pastel\\\")\\n",
                    "plt.rcParams['figure.figsize'] = (10, 6)\\n\\n",
                    "print(\\\"‚úÖ Bibliotecas importadas com sucesso!\\\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "carregar-dados",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- CARREGAR DADOS ---\\n",
                    "print(\\\"üì• Carregando dataset...\\\")\\n",
                    "df = pd.read_csv(\\\"dataset_explorado.csv\\\")\\n\\n",
                    "print(f\\\"üìä Dimens√µes iniciais: {df.shape}\\\")\\n",
                    "print(f\\\"üîç Valores faltantes iniciais: {df.isna().sum().sum()}\\\")\\n\\n",
                    "# Visualizar primeiras linhas\\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown", 
                "id": "valores-faltantes",
                "metadata": {},
                "source": [
                    "## 1. üßπ TRATAMENTO DE VALORES FALTANTES"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "tratamento-faltantes",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- TRATAMENTO DE VALORES FALTANTES ---\\n",
                    "print(\\\"üîç Tratando valores faltantes...\\\")\\n\\n",
                    "# Estrat√©gia direta e agressiva\\n",
                    "for coluna in df.columns:\\n",
                    "    if df[coluna].isna().sum() > 0:\\n",
                    "        if df[coluna].dtype in ['int64', 'float64']:\\n",
                    "            # Num√©ricas: mediana\\n",
                    "            valor = df[coluna].median()\\n",
                    "            df[coluna] = df[coluna].fillna(valor)\\n",
                    "        else:\\n",
                    "            # Categ√≥ricas: moda ou 'MISSING'\\n",
                    "            if len(df[coluna].mode()) > 0:\\n",
                    "                valor = df[coluna].mode()[0]\\n",
                    "            else:\\n",
                    "                valor = 'MISSING'\\n",
                    "            df[coluna] = df[coluna].fillna(valor)\\n",
                    "        print(f\\\"‚úÖ {coluna}: {df[coluna].isna().sum()} faltantes restantes\\\")\\n\\n",
                    "print(f\\\"üéØ Valores faltantes totais: {df.isna().sum().sum()}\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "encoding-categorico", 
                "metadata": {},
                "source": [
                    "## 2. üîÑ ENCODING DE VARI√ÅVEIS CATEG√ìRICAS"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "aplicar-encoding",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- ENCODING CATEG√ìRICO ---\\n",
                    "print(\\\"üîç Aplicando encoding categ√≥rico...\\\")\\n\\n",
                    "# One-Hot Encoding\\n",
                    "categoricas = ['product_category', 'payment_methods']\\n",
                    "for col in categoricas:\\n",
                    "    if col in df.columns:\\n",
                    "        df = pd.get_dummies(df, columns=[col], drop_first=True, prefix=col)\\n\\n",
                    "# Encoding ordinal\\n",
                    "map_ordinal = {'Low': 0, 'Medium': 1, 'High': 2}\\n",
                    "df['competition_level'] = df['competition_level'].map(map_ordinal)\\n",
                    "df['seasonality'] = df['seasonality'].map(map_ordinal)\\n\\n",
                    "# Encoding bin√°rio\\n",
                    "df['free_shipping'] = df['free_shipping'].map({'Yes': 1, 'No': 0})\\n\\n",
                    "print(f\\\"‚úÖ Encoding aplicado! Novo formato: {df.shape}\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "tratamento-outliers",
                "metadata": {},
                "source": [
                    "## 3. üìä TRATAMENTO DE OUTLIERS"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "aplicar-outliers",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- TRATAMENTO DE OUTLIERS ---\\n",
                    "print(\\\"üîç Tratando outliers...\\\")\\n\\n",
                    "numericas = ['marketing_spend', 'website_traffic', 'avg_price', 'conversion_rate']\\n\\n",
                    "for col in numericas:\\n",
                    "    if col in df.columns:\\n",
                    "        Q1, Q3 = df[col].quantile([0.25, 0.75])\\n",
                    "        IQR = Q3 - Q1\\n",
                    "        limite_inf = Q1 - 1.5 * IQR\\n",
                    "        limite_sup = Q3 + 1.5 * IQR\\n\\n",
                    "        # Aplicar limites\\n",
                    "        df[col] = np.clip(df[col], limite_inf, limite_sup)\\n",
                    "        print(f\\\"‚úÖ {col}: outliers tratados\\\")\\n\\n",
                    "print(\\\"üéØ Outliers tratados com sucesso!\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "normalizacao",
                "metadata": {},
                "source": [
                    "## 4. ‚öñÔ∏è NORMALIZA√á√ÉO (SCALING)"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "aplicar-normalizacao",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- NORMALIZA√á√ÉO ---\\n",
                    "print(\\\"üîç Aplicando normaliza√ß√£o...\\\")\\n\\n",
                    "colunas_normalizar = [\\n",
                    "    'marketing_spend', 'website_traffic', 'conversion_rate',\\n",
                    "    'avg_product_rating', 'avg_price', 'customer_reviews',\\n",
                    "    'return_rate', 'monthly_sales'\\n",
                    "]\\n\\n",
                    "# Filtrar colunas existentes\\n",
                    "colunas_normalizar = [col for col in colunas_normalizar if col in df.columns]\\n\\n",
                    "# Aplicar scaler\\n",
                    "scaler = StandardScaler()\\n",
                    "df[colunas_normalizar] = scaler.fit_transform(df[colunas_normalizar])\\n\\n",
                    "# Salvar scaler\\n",
                    "os.makedirs(\\\"models\\\", exist_ok=True)\\n",
                    "joblib.dump(scaler, \\\"models/scaler.pkl\\\")\\n\\n",
                    "print(f\\\"‚úÖ Normaliza√ß√£o aplicada em {len(colunas_normalizar)} colunas\\\")\\n",
                    "print(\\\"üíæ Scaler salvo em 'models/scaler.pkl'\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "feature-engineering",
                "metadata": {},
                "source": [
                    "## 5. üéØ FEATURE ENGINEERING"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "criar-features",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- FEATURE ENGINEERING ---\\n",
                    "print(\\\"üîç Criando novas features...\\\")\\n\\n",
                    "df['marketing_eficiencia'] = df['monthly_sales'] / (df['marketing_spend'] + 1)\\n",
                    "df['traffic_conversion'] = df['website_traffic'] * df['conversion_rate']\\n",
                    "df['price_rating_ratio'] = df['avg_price'] / (df['avg_product_rating'] + 1)\\n",
                    "df['customer_value'] = df['monthly_sales'] / (df['customer_reviews'] + 1)\\n\\n",
                    "print(\\\"‚úÖ 4 novas features criadas!\\\")\\n\\n",
                    "# Mostrar estat√≠sticas das novas features\\n",
                    "novas_features = ['marketing_eficiencia', 'traffic_conversion', 'price_rating_ratio', 'customer_value']\\n",
                    "for feature in novas_features:\\n",
                    "    if feature in df.columns:\\n",
                    "        print(f\\\"üìä {feature}: mean={df[feature].mean():.4f}, std={df[feature].std():.4f}\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "salvamento",
                "metadata": {},
                "source": [
                    "## 6. üíæ SALVAMENTO FINAL"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "salvar-dataset",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- SALVAR DATASET FINAL ---\\n",
                    "df.to_csv(\\\"dataset_preprocessado.csv\\\", index=False)\\n\\n",
                    "print(\\\"üíæ Dataset salvo como 'dataset_preprocessado.csv'\\\")\\n",
                    "print(f\\\"üìä Dimens√µes finais: {df.shape}\\\")\\n",
                    "print(f\\\"üîç Valores faltantes finais: {df.isna().sum().sum()}\\\")"
                ]
            },
            {
                "cell_type": "markdown",
                "id": "verificacao-final",
                "metadata": {},
                "source": [
                    "## 7. ‚úÖ VERIFICA√á√ÉO FINAL"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "id": "verificacao-completa",
                "metadata": {},
                "outputs": [],
                "source": [
                    "# --- VERIFICA√á√ÉO FINAL ---\\n",
                    "print(\\\"üéØ VERIFICA√á√ÉO FINAL:\\\")\\n",
                    "print(\\\"=\" * 50)\\n\\n",
                    "# 1. Valores faltantes\\n",
                    "missing_final = df.isna().sum().sum()\\n",
                    "print(f\\\"üîç Valores faltantes: {'‚úÖ ZERO' if missing_final == 0 else f'‚ùå {missing_final}'}\\\")\\n\\n",
                    "# 2. Dimens√µes\\n",
                    "print(f\\\"üìä Formato do dataset: {df.shape}\\\")\\n\\n",
                    "# 3. Normaliza√ß√£o\\n",
                    "if colunas_normalizar:\\n",
                    "    mean_check = df[colunas_normalizar].mean().abs().max()\\n",
                    "    std_check = df[colunas_normalizar].std().mean()\\n",
                    "    print(f\\\"üìà M√©dias ap√≥s scaling: {mean_check:.4f} (deve ser ~0)\\\")\\n",
                    "    print(f\\\"üìà Desvio padr√£o m√©dio: {std_check:.4f} (deve ser ~1)\\\")\\n\\n",
                    "# 4. Arquivos salvos\\n",
                    "scaler_exists = os.path.exists(\\\"models/scaler.pkl\\\")\\n",
                    "dataset_exists = os.path.exists(\\\"dataset_preprocessado.csv\\\")\\n",
                    "print(f\\\"üíæ Scaler salvo: {'‚úÖ SIM' if scaler_exists else '‚ùå N√ÉO'}\\\")\\n",
                    "print(f\\\"üíæ Dataset salvo: {'‚úÖ SIM' if dataset_exists else '‚ùå N√ÉO'}\\\")\\n\\n",
                    "# 5. Novas features\\n",
                    "novas_features = ['marketing_eficiencia', 'traffic_conversion', 'price_rating_ratio', 'customer_value']\\n",
                    "features_count = sum([1 for f in novas_features if f in df.columns])\\n",
                    "print(f\\\"üÜï Features criadas: {features_count}/{len(novas_features)}\\\")\\n\\n",
                    "print(\\\"=\" * 50)\\n\\n",
                    "# Verifica√ß√£o de sucesso\\n",
                    "if missing_final == 0 and scaler_exists and dataset_exists and features_count == len(novas_features):\\n",
                    "    print(\\\"\\\\nüéä PR√â-PROCESSAMENTO CONCLU√çDO COM SUCESSO!\\\")\\n",
                    "    print(\\\"üöÄ O dataset est√° pronto para modelagem!\\\")\\n",
                    "else:\\n",
                    "    print(\\\"\\\\n‚ö†Ô∏è  ALGUMAS VERIFICA√á√ïES FALHARAM!\\\")\\n\\n",
                    "print(\\\"\\\\nüìã RESUMO FINAL:\\\")\\n",
                    "print(f\\\"   ‚Ä¢ Features finais: {len(df.columns)}\\\")\\n",
                    "print(f\\\"   ‚Ä¢ Linhas processadas: {df.shape[0]}\\\")\\n",
                    "print(f\\\"   ‚Ä¢ Novas features: +{features_count}\\\")\\n",
                    "print(\\\"   ‚Ä¢ Arquivo: dataset_preprocessado.csv\\\")"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
    
    # Salvar como arquivo .ipynb
    with open('02_Preprocessamento.ipynb', 'w', encoding='utf-8') as f:
        json.dump(notebook_content, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ NOTEBOOK CRIADO COM SUCESSO!")
    print("üìì Arquivo: 02_Preprocessamento.ipynb")
    print("üöÄ Execute no Jupyter e siga as c√©lulas!")

if __name__ == "__main__":
    create_preprocessing_notebook()